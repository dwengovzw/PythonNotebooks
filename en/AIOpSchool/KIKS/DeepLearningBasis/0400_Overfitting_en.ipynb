{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/kiksmeisedwengougent.png\" alt=\"Banner\" width=\"1100\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <font color=#690027 markdown=\"1\">\n",
    "<h1>OVERFITTING</h1>    </font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following code cells to be able to use the necessary functions in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib.util\n",
    "spec = importlib.util.spec_from_file_location(\n",
    "name = \"deep_neural_network\",\n",
    "location = \".scripts/diep_neuraal_netwerk.py\"\n",
    ")\n",
    "deep_neural_network = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(deep_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <font color=#690027 markdown=\"1\">\n",
    "<h2>1. What is overfitting</h2>    </font>\n",
    "</div>\n",
    "\n",
    "When a network becomes too fixated on the training data, it will perform less well on unseen data. The network then, as it were, memorizes the training data. This is called ***overfitting***. <br>When the network can still learn, for example by adding more layers, this is called ***underfitting***.\n",
    "\n",
    "The following figure visually represents both concepts. A deep neural network is essentially looking for a function that maps the input to the correct output and generalizes well enough.\n",
    "<img src=\"images/overfitting.jpg\" width=\"600\"/>\n",
    "\n",
    "A network can also overfit on the validation data. Based on the performance of the network on the validation data, you will modify the code behind the network to find the best network for your problem. Every time you modify the network based on the validation data, the network learns a little bit about this data. The final network will therefore perform better on this data than on 'real' unseen data. That's why other data (the test data) is used to measure the performance of this final network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <font color=#690027 markdown=\"1\">\n",
    "<h2>2. Counteracting overfitting</h2>    </font>\n",
    "</div>\n",
    "\n",
    "Overfitting is one of the most common problems when building a deep neural network. Luckily, there are various techniques to counteract overfitting. These techniques are also called <b>regularization</b> techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <font color=#690027 markdown=\"1\">\n",
    "<h3>2.1 More training data</h3>    </font>\n",
    "</div>\n",
    "\n",
    "The more training data there is, the less likely it is that the model will overfit quickly. The reason for this can be explained using an example.\n",
    "\n",
    "Suppose you want to train a network that should be able to predict whether a man or a woman is in a passport photo. Coincidentally, all the men in the training data have a beard. The network might draw the wrong conclusion that everyone with a beard is a man and everyone without a beard is a woman (the network overfits the training data). When we expand the training data to also include photos of men without a beard, the network will be less likely to draw that wrong conclusion again.\n",
    "\n",
    "With more data, the data will most likely also be more varied. A problem like the one described above will then be less likely to occur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <font color=#690027 markdown=\"1\">\n",
    "<h3>2.2 Simpler network</h3>    </font>\n",
    "</div>\n",
    "\n",
    "A complex network with many layers and many weights will be able to store a lot of information about the training data and thus learn unimportant details from this data by heart more quickly, this leads to overfitting. A simple network (with fewer layers and neurons) will be able to store less information and is therefore forced to only recognize the most prominent features from the training data.\n",
    "\n",
    "A method to combat overfitting is therefore to start with a simple network and make this network increasingly complex (adding layers or neurons) until you notice that it is going to overfit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <font color=#690027 markdown=\"1\">\n",
    "<h3>2.3 Data Augmentation</h3>    </font>\n",
    "</div>\n",
    "\n",
    "The more training data the better, but sometimes you don't have a lot of training data. With *data augmentation* you can modify the available training data so that the network always sees different data. For images, for example, you can rotate the image a certain number of degrees or you can mirror the image relative to the x or y-axis. You can adjust the intensity of the colors or shift the image to the left, right, top or bottom, and so on.\n",
    "\n",
    "The following image shows various possibilities of data augmentation on an image of a stoma.\n",
    "<img src=\"images/dataaugmentatie.jpg\" width=\"600\"/>\n",
    "\n",
    "The networks used in this notebook employ the following data augmentation to combat overfitting:\n",
    "<ul>\n",
    "<li><b>Horizontal mirroring</b>: It is randomly decided whether or not to mirror over the y-axis.</li><li><b>Vertical mirroring</b>: It is randomly determined whether or not there will be a mirror along the x-axis.</li><li><b>Rotation</b>: A number is randomly chosen between 0 and 180, then the image is rotated by that number of degrees. If in this way there are too few pixels (in the corners), these are supplemented by repeating the nearest pixel.</li></ul>\n",
    "\n",
    "Shifting the image would not be a good idea for the KIKS network because the network needs to look for a stoma in the middle of the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <font color=#690027 markdown=\"1\">\n",
    "<h3>2.4 Dropout</h3>    </font>\n",
    "</div>\n",
    "\n",
    "A very efficient and widely used method of regularization is the addition of *dropout*. As mentioned in the notebook 'Fundamentals of a deep neural network for image recognition', a *feedforward* layer consists of neurons. Dropout will ensure that a random part of these neurons no longer output (this corresponds to an output of 0) and thus will not activate any other neurons from a subsequent layer, regardless of the weights of the connections between these neurons. The percentage of neurons whose output is reduced to 0 is called the <b>dropout rate</b>. By always choosing different neurons when the network processes an image, the network will less easily memorize the training data but will still get enough information to learn relevant patterns from the training data.\n",
    "\n",
    "The following image shows a network without dropout and a network where dropout has been added to the two middle feedforward layers with a dropout rate of 0.5. The circles represent the neurons and the lines the weighted connections between the different layers.\n",
    "<img src=\"images/dropout.jpg\" width=\"600\"/>\n",
    "\n",
    "Dropout is only used during the training of the network, when the network is tested/used, all neurons will therefore work normally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <font color=#690027 markdown=\"1\">\n",
    "<h2>3. Training the network with regularization</h2>    </font>\n",
    "</div>\n",
    "\n",
    "As an exercise, you assemble a network again. However, this time, the data augmentation of the training data described above is used, and after each feedforward layer, dropout is added with a dropout rate of 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "deep_neural_network.kies_netwerk_parameters()\n",
    "deep_neural_network.kies_training_parameters()\n",
    "deep_neural_network.update_model(\"regularisatie\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After choosing the network architecture and the training parameters, you can visualize the network with the following code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "deep_neural_network.toon_netwerk()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <font color=#690027 markdown=\"1\">\n",
    "<h2>4. Results</h2>    </font>\n",
    "</div>\n",
    "\n",
    "Overfitting is best observed on the graph of *loss* values over different *epochs*. When the training loss decreases but the validation loss starts to increase, there is overfitting. However, a network with regularization will (normally) only start overfitting later or not overfit at all (the validation loss remains approximately the same)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 4.1\n",
    "Choose different networks by adjusting the parameters above. Determine which networks are overfitting and which are not. Execute the following code cell for this. <br>When you adjust the parameters of the network, you need to re-run the code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Chosen network with regularization\\n\")\n",
    "deep_neural_network.toon_grafiek()\n",
    "\n",
    "print(\"Chosen network without regularization\\n\")\n",
    "deep_neural_network.update_model(\"regularisatie\", False)\n",
    "deep_neural_network.toon_grafiek()\n",
    "deep_neural_network.update_model(\"regularisatie\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/cclic.png\" alt=\"Banner\" align=\"left\" width=\"100\"/><br><br>\n",
    "Notebook KIKS, see <a href=\"http://www.aiopschool.be\">AI At School</a>, from F. Wyffels, A. Meheus, T. Neutens & N. Gesquière, is licensed under a <a href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<h2>With support from</h2></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/kikssteun.png\" alt=\"Banner\" width=\"1100\"/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
