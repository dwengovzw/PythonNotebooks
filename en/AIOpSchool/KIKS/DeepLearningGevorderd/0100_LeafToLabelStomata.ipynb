{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style='color: #690027;' markdown=\"1\">\n",
        "<h1>From leaf to label: stomata detection</h1>",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This notebook allows you to upload and test your own photomicrographs with the deep learning system discussed in the paper *From leaf to label: a robust automated workflow for stomata detection* by *Sofie Meeus, Jan Van den Bulcke and Francis Wyffels*.",
        "<img src=\"https://drive.google.com/uc?export=view&id=1g1wIt37A07yDi7w9uCza3eMt1oEPLJ5v\" alt=\"Overview\" style=\"width:600px;\"/>\n",
        "As illustrated above, your photomicrograph (A) will be divided into small overlapping patches (B) using a sliding window approach. A deep neural network (VGG19) is trained to label these patches (C). Positively labeled patches of a photomicrograph (D) are clustered which results in the detection (E) depending on the threshold of your choice.",
        "\n",
        "To begin, please run the following cell by clicking the \"run\" button or by using shift-enter.",
        "\n",
        "Before starting, if you run this notebook on Colab you might want GPU acceleration. Therefore click: *Edit* > *Notebook Settings* > *Hardware accelerator GPU* > *Save*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load libraries",
        "\n",
        "We start by loading a few python libraries:",
        "\n",
        "- [PIL](https://pillow.readthedocs.io/en/stable/): a convenient Python imaging library",
        "- [numpy](https://numpy.org): het fundamentele pakket voor wetenschappelijk rekenen met Python",
        "- [sklearn](https://scikit-learn.org/stable/): the scikit-learn machine learning package, more specifically the clustering functionality",
        "- [os](https://docs.python.org/3/library/os.html): a Python library for using operating system dependent functionality, e.g., reading, writing, listing files",
        "- [matplotlib](https://matplotlib.org): a Python library for making graphs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from PIL import Image, ImageEnhance, ImageOps",
        "import numpy as np",
        "from sklearn.cluster import MeanShift, estimate_bandwidth",
        "import os",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Om te specificeren welke NVidia GPU we zullen gebruiken, moeten we de volgende commando's uitvoeren. Dit gaat ervan uit dat Keras en Tensorflow GPU-enabled zijn. Bekijk de [TensorFlow](https://www.tensorflow.org/install/gpu) documentatie voor meer informatie."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# If there is a NVidia enabled GPU, run this",
        "# os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"",
        "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The code that produced the results presented in the manuscript is based on Tensorflow and Keras. At the time of the writing, TensorFlow 2.0 was not yet available. However, since we relied on a TF 1.0 and Keras implementation. However, the principles for training your own deep learning model stay the same and should be sufficient to get started with [Keras](https://keras.io/getting_started/intro_to_keras_for_researchers/) yourself."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras import backend as K",
        "from tensorflow.keras.layers import Input, Convolution2D, Conv2D, MaxPooling2D, Activation, concatenate, Dropout, GlobalAveragePooling2D, Flatten, Dense",
        "from tensorflow.keras.models import Model, load_model",
        "# from keras.engine.topology import get_source_inputs",
        "from tensorflow.keras.utils import get_source_inputs",
        "from tensorflow.keras.utils import get_file",
        "from tensorflow.python.keras.utils import layer_utils",
        "# import keras",
        "import tensorflow.keras as keras",
        "from tensorflow.keras.preprocessing.image import load_img",
        "from tensorflow.keras.preprocessing.image import img_to_array",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator",
        "import tensorflow as tf",
        "\n",
        "# limiting GPU VRAM",
        "config = tf.compat.v1.ConfigProto()",
        "config.gpu_options.allow_growth = True",
        "sess = tf.compat.v1.Session(config=config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Set data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To train a deep learning model, you need data. As previously discussed, the deep learning model we are discussing here will be trained based on rectangular patches. In order to get a robust model, both positive and negative examples must be presented to the system.",
        "\n",
        "The data needs to be split into three parts:",
        "- The training set, i.e., the data which is used for changing the weights of the (deep) neural network;",
        "- The validation, i.e., the data which is used to see how well the learning process is going and to tune the model's hyperparameters;",
        "- The test data, i.e., the data you feed to the trained system afterwards when the detector is deployed.",
        "\n",
        "This notebook concerns the training and validation of the deep learning system for stomata detection, and a small dataset limited to *Carapa procera* is used for didactic purposes. This also limits the computational needs (a full training with multiple species (cf. the paper) requires more patience."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We start by downloading and unzipping the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!wget https://zenodo.org/record/3902280/files/data.zip",
        "!unzip 'data.zip'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_dir = \"./data/training/\"",
        "val_dir = \"./data/validation/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The training and validation data consists of patches of 120 by 120 pixels. A positive patch shows a stomata:",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=11oZG14b8ZnbzooeeQpYaQeUe8wSbErmV\"  width=\"120\" />\n",
        "    \n",
        "A negative patch of *Carapa procera* has no stomata (or only partially) within the patch:",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1vwcYiZDJffjQ6gXCpThwvbcCoPhgseBS\"  width=\"120\" />\n",
        "\n",
        "In order to obtain these patches you will need labeled microphotographs (i.e., microphotographs of which you have the x,y coordinates of the center position of the stomata that are shown). Based on these labels, the patches can be cropped by using the [crop function of PIL](https://pillow.readthedocs.io/en/stable/reference/Image.html) or simply by [matrix slicing](https://numpy.org/doc/1.18/reference/arrays.indexing.html) in Python Numpy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The preprocessor defines the data augmentation that will be applied to the dataset. This consists of random rotations, horizontal and vertical flips of the patches:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_datagen = keras.preprocessing.image.ImageDataGenerator(rotation_range=180, horizontal_flip=True, vertical_flip=True, rescale=1/255.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Apart from the data augmentation that will be applied, we need to set a [ImageDataGenerator](https://keras.io/api/preprocessing/image/#imagedatagenerator-class) that defines the size of the patches (120 x 120 pixels), the color mode, the batch size (i.e., the number of samples used in one training iteration), the classification mode of the task (i.e., binary classification: a patch can be positive or negative), whether the data needs to be shuffled or not and the seed (starting point) of the random number generator. Additionally, you need to provide a path to the directory with the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "batch_size = 128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_generator = train_datagen.flow_from_directory(",
        "    directory=r\"./data/training/\",",
        "    target_size=(120, 120),",
        "    color_mode=\"rgb\",",
        "batch_size=batch_size,",
        "    class_mode=\"binary\",",
        "    shuffle=True,",
        "seed=53",
        "The assistant didn't receive any text to translate. Your instruction suggests that you were supposed to provide some Dutch text for translation. You might want to try again."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Voor validatie moet je ook een ImageDataGenerator leveren. Dit is gedefinieerd met dezelfde eigenschappen als die voor training, maar zonder data-augmentatie."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_datagen = ImageDataGenerator(rescale=1/255.)",
        "\n",
        "validation_generator = test_datagen.flow_from_directory(",
        "r\"./data/validation/\",",
        "        target_size=(120, 120),",
        "        color_mode=\"rgb\",",
        "batch_size=batch_size,",
        "class_mode='binary')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Set model parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We start from a [VGG19](https://arxiv.org/abs/1409.1556) with two dense layers on top. The convolutional neural layers are pre-trained on [ImageNet](https://ieeexplore.ieee.org/abstract/document/5206848). Consequently, only the dense layers are trained. These pre-trained weights can be downloaded from Keras by adding the keyword \"imagenet\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "number_dense_neurons = 2048"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# VGG19 Fine Tuned",
        "from tensorflow.keras.applications import VGG19",
        "# We start from a VGG19 base (convolutional neural layers) with weights pre-trained on ImageNet",
        "vgg19_base = VGG19(weights='imagenet',include_top=False,input_shape=(120,120,3))",
        "x = vgg19_base.output",
        "x = Flatten()(x)",
        "# We add our own densely layered classifier on top",
        "x = Dense(2*number_dense_neurons,activation='relu')(x)",
        "x = Dropout(0.5)(x)",
        "x = Dense(number_dense_neurons,activation='relu')(x)",
        "x = Dropout(0.5)(x)",
        "# Output layer",
        "x = Dense(1,activation='sigmoid')(x)",
        "\n",
        "model = Model(inputs=vgg19_base.input, outputs=x)",
        "\n",
        "# Only the dense layers are trained, hence, the convolutional neural layers are set not trainable:",
        "for layer in vgg19_base.layers:",
        "    layer.trainable = False",
        "\n",
        "# What does the network look like",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "These parameters were optimized by using the [Adam](https://arxiv.org/pdf/1412.6980.pdf) learning rule for which the learning rate was tuned and finally set to 0.000005. Additionally, we need to configure the training loss and metrics for validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "learning_rate = 0.000005",
        "# Initiate Stochastic Gradient Descent with momentum, learning rate to tune",
        "opt = tf.keras.optimizers.Adam(lr=learning_rate, beta_1=0.9, beta_2=0.999, amsgrad=False)",
        "# Define the losses and metrics for validation",
        "model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['binary_accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Training can be done by using the function *fit*. We train for 50 epochs. Note that we configured our architecture in a way that only the weights of the dense layers are adjusted."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "epochs = 15",
        "The input provided does not contain any Dutch text. It appears to be a part of a Python code which does not need translation.",
        "train_generator,",
        "        epochs=epochs,",
        "        validation_data=validation_generator)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Nu zijn we klaar, ons netwerk is getraind en kan in gebruik worden genomen. Om je systeem te kunnen gebruiken, moet je de modelparameters opslaan. Dit kan door *model.save(pad)* aan te roepen met *pad*, het pad naar het bestand waarin je de parameters wilt opslaan. Bovendien geeft de *fit* een *history* object terug. Dit registreert de trainings- en validatievoortgang over de epochs. Dit is dan ook handig om de trainingsvoortgang bij te houden, bijvoorbeeld bij het vergelijken van meerdere hyperparameterinstellingen."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the Carapa procera deep learning model",
        "model.save('my_carapa_procera_model')",
        "\n",
        "# Plot the training and validation losses over time",
        "plt.plot(history.history['loss'], label='Training loss')",
        "plt.plot(history.history['val_loss'], label='Validation loss')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load a deep learning model",
        "\n",
        "Now we have trained a first deep learning model on the *Carapa procera*. This is stored in the object *model*. If you want to start from a saved deep learning model, you simply load it from a file by calling: *model = load_model(path_to_model)*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Image and detection parameters",
        "\n",
        "Het model maakt gebruik van een schuifvensterbenadering. Hoewel niet het meest (computationeel) effici\u00ebnt, is het heel eenvoudig te begrijpen. Het venster heeft een grootte van 120 bij 120 pixels en we gebruiken een stap van 10 pixels. We beginnen met het laden van onze afbeelding:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "demo_image = './data/Carapa_procero_demo.jpg' # you can use any other Carapa procero microphotograph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "image = Image.open(demo_image)",
        "fig, ax = plt.subplots(figsize=(20, 10))",
        "image = np.array(image) # conversion to a Numpy array",
        "ax.imshow(image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "shift = 10",
        "patch_size = 120"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Also part of the detection parameters is the number of slides we will do:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "no_x_shifts = (np.shape(image)[0] - patch_size) // shift",
        "no_y_shifts = (np.shape(image)[1] - patch_size) // shift",
        "print(\"We will do \"+str(no_x_shifts*no_y_shifts)+\" slides. Consequently, the deep learning model will be applied to \"+str(no_x_shifts*no_y_shifts)+\" windows.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Classification with a deep learning model",
        "\n",
        "Now that we have identified all the windows, we apply the deep learning model. This is done by calling the *predict* function. The image we apply needs to be converted and normalized before applying the deep learning model. Since the output of the deep learning model is between 0 and 1, we also have to set a threshold from which we accept the output as a positive classification. The higher this threshold, the more precise the system will be in detecting stomata. However, if the threshold is too high, the system won't be able to detect stomata at all. Here we use the same threshold as determined in the paper:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "threshold = 0.7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "stomata = []",
        "offset = patch_size // 2",
        "for x in np.arange(no_x_shifts + 1):",
        "    for y in np.arange(no_y_shifts + 1):",
        "# center of the window",
        "x_c = x * shift + offset",
        "        y_c = y * shift + offset",
        "        \n",
        "# extraction of the window and conversion before applying the deep learning model",
        "patch = image[x_c - offset:x_c + offset, y_c - offset:y_c + offset, :]",
        "        patch = patch.astype('float32')",
        "patch /= 255",
        "        \n",
        "# applying the deep learning model",
        "y_model = model.predict(np.expand_dims(patch, axis=0))",
        "\n",
        "# if the output of the network is above the",
        "        if y_model[0] > threshold:",
        "stomata.append([x_c, y_c])",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "patches = []",
        "coordinates = []",
        "stomata = []",
        "offset = patch_size // 2",
        "for x in np.arange(no_x_shifts + 1):",
        "for y in np.arange(no_y_shifts + 1):",
        "# Center of the window",
        "        x_c = x * shift + offset",
        "y_c = y * shift + offset",
        "\n",
        "# Cutting out the window and converting to expected format before applying the deep learning model",
        "patch = image[x_c - offset:x_c + offset, y_c - offset:y_c + offset, :]",
        "patch = patch.astype(\"float32\")",
        "patch /= 255",
        "        \n",
        "# Add window to the list of windows",
        "        patches.append(np.expand_dims(patch, axis=0))",
        "        \n",
        "# Add coordinates of the center of the window to the list of coordinates",
        "        coordinates.append([x_c, y_c])",
        "\n",
        "# Process all windows in batches of 128 windows to carry out calculations in parallel",
        "batch_size = 128",
        "for b in range(0, len(patches), batch_size):",
        "    batch = patches[b:b + batch_size]",
        "\n",
        "# Apply the model to perform the detection",
        "y_model = model.predict(np.vstack(batch))",
        "\n",
        "for p in range(0, len(y_model)):",
        "# Stoma if the output of the model lies above the threshold",
        "        if y_model[p] > threshold:",
        "stomata.append(coordinates[b + p])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Clustering the detected stomata",
        "\n",
        "All positively labeled patches are clustered using mean shift clustering. This technique groups neighboring (or even overlapping) positively labeled patches from which the resulting stoma coordinates are derived. Therefore, we can rely on the package [MeanShift](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.MeanShift.html) which is available in [scikit-learn](https://scikit-learn.org)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "bandwidth = patch_size // 2",
        "\n",
        "ms = MeanShift(bandwidth=bandwidth, bin_seeding=True)",
        "ms.fit(stomata)",
        "stomata = np.array([[x[1], x[0]] for x in ms.cluster_centers_]) # cluster_centers_ is inverted"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Plotting the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(20, 10))",
        "ax.imshow(image)",
        "ax.plot(stomata[:,0], stomata[:,1], 'xr', alpha=0.75, markeredgewidth=3, markersize=12)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}