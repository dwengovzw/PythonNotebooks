{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src=\"images/bannerugentdwengo.png\" alt=\"BannerUGentDwengo\" width=\"250\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div>\n",
        "    <font color=#690027 markdown=\"1\">\n",
        "<h1>AI MODEL FOR SENTIMENT ANALYSIS</h1>",
        "    </font>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-box alert-success\">\n",
        "In this notebook, you will investigate sentiment words in given texts (the data) using artificial intelligence (AI). After all, you will be using a <em>machine learning</em> model. This model was trained with annotated texts and can tokenize a text with high accuracy and determine the part-of-speech tag and the lemma of each token. You use a <em>rule-based AI system</em> to determine the sentiment of the given text.",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the previous notebook, 'Rule-based sentiment analysis', you were introduced to the principles of rule-based sentiment analysis:",
        "\n",
        "- You use an (existing) **lexicon** or dictionary that links words to their **polarity** (positive, negative, or neutral).",
        "- Before you can match sentiment words from a lexicon with the data, you need to read in the data and **preprocess** it.",
        "- Common preprocessing steps are **lowercasing**, **tokenization**, **part-of-speech tagging**, and **lemmatization**.",
        " \n",
        "In the previous notebook, not all steps were automated. Lemmatization and part-of-speech tagging had to be done manually."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-box alert-success\">\n",
        "In this notebook, you will fully <b>automate</b> the output of sentiment analysis. In other words, you will let the computer do the work: the computer will preprocess the data with a <em>machine learning model (ML-model)</em>, and match the tokens with the given lexicon using a <em>rule-based AI system</em>, and finally make a decision on the sentiment of the given text.",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Loading Modules, Model and Lexicon"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before you get started, first provide the necessary tools:",
        "\n",
        "- You import the necessary modules (you only need to do this once). <br>These modules contain functions and methods that will facilitate your research. There are in fact already things pre-programmed, allowing you to work with quite simple instructions.",
        "- You load a machine learning model to use later.",
        "- You're also reading in a sentiment lexicon.",
        "\n",
        "To do this, execute the three code cells below. You do not need to understand the code in these cells."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import modules",
        "import pickle                     # for lexicon",
        "from colorama import Fore, Back   # to be able to display in color",
        "import spacy                      # for machine learning preprocessing model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# load machine learning model",
        "nlp = spacy.load(\"nl_core_news_sm\")    # nlp stands for Natural Language Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# read in lexicon, file 'new_lexicondict.pickle' contains sentiment lexicon",
        "with open(\"data/new_lexicondict.pickle\", \"rb\") as file:",
        "    lexicon = pickle.load(file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So, you're ready for step 1: read in and review the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div>\n",
        "    <font color=#690027 markdown=\"1\">\n",
        "<h2>1. Read the data</h2>",
        "    </font>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For this task, you will work with the same **customer review** as in the notebook 'Rule-based sentiment analysis'."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Step 1: Execute the following code cell to read in the review and then view it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "review = \"New concept in Ghent, but I think it could be better. Most of the cornflakes were just the basic types. Also a bit expensive for the amount you get, especially with the toppings they are stingy. And if you offer breakfast, at least give people a bit more choice for their coffee.\"",
        "print(review)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You are ready for step 2.",
        "\n",
        "In the following, you let the computer do all the pre-processing on the review: we had already automated lowercasing in 'Rule-based Sentiment Analysis'. You take over that code.",
        "\n",
        "You should not add spaces in the text, because the machine learning model takes care of the tokenization. Also, part-of-speech tagging and lemmatization are now automated with the help of the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div>\n",
        "    <font color=#690027 markdown=\"1\">\n",
        "<h2>2. Preprocessing</h2>",
        "    </font>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Lowercasing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# convert review text to lowercase text",
        "review_lowercase = review.lower()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Tokenization, part-of-speech tagging, and lemmatization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The review is **tokenized** and each token is assigned a **part-of-speech** and a **lemma**, this happens automatically with the help of a previously trained model with an accuracy of 93%!",
        "\n",
        "For this, you enter the review (in lowercase) in the ML model `nlp`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# input review in lowercase into model",
        "review_preprocessed = nlp(review_lowercase)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The tokens of the review have now been determined and for each token, the word (or punctuation) itself, the part-of-speech tag, and the dictionary form (lemma) are stored in an object referred to by `review_preprocessed`. <br>",
        "You can now request the characteristics of tokens: the word/punctuation via the instruction `token.text`, the part of speech via `token.pos_` and the dictionary form via `token.lemma_`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Show the part of speech and dictionary form of each token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# tokens",
        "for token in review_preprocessed:",
        "print(token.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# part-of-speech tag of each token",
        "for token in review_preprocessed:",
        "print(token.text + \": \" + token.pos_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# lemma of each token",
        "for token in review_preprocessed:",
        "print(token.text + \": \" + token.lemma_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create lists of the tokens, lemmas, and part-of-speech tags."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In 'Rule-based sentiment analysis', the lists of lemmas and part-of-speech tags were manually created. Now, this can be done automatically because all the necessary information is collected in the object to which the variable `review_voorverwerkt` refers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# listing",
        "tokens = []",
        "lemmas = []",
        "postags = []",
        "for token in review_preprocessed:",
        "    tokens.append(token.text)      # add each token to the list of tokens",
        "lemmas.append(token.lemma_)    # add each lemma to the list of lemmas",
        "postags.append(token.pos_)     # add each part-of-speech tag to list of postags",
        "\n",
        "# showing lists",
        "print(\"tokens:\")",
        "print(tokens)",
        "print(\"lemmas:\")",
        "print(lemmas)",
        "print(\"part-of-speech tags:\")",
        "print(postags)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div>\n",
        "    <font color=#690027 markdown=\"1\">\n",
        "<h2>3. Sentiment lexicon matching</h2>",
        "    </font>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that the review has been *preprocessed*, you can determine the sentiment using the sentiment lexicon that you have available. This was already automated in 'Rule-based sentiment analysis'. You take over the code from 'Rule-based sentiment analysis'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# search for matches with lexicon in review",
        "lexiconmatches = []       # empty list, to be filled with tokens of the lemmas found in lexicon",
        "polarities = []         # empty list, to be filled with polarities of found tokens",
        "\n",
        "# consider lemmas with corresponding word type and token",
        "for lemma, postag, token in zip(lemmas, postags, tokens):",
        "    if lemma in lexicon.keys() and postag in lexicon[lemma][\"postag\"]:",
        "lexiconmatches.append(token)                      # add corresponding token to lexiconmatches list",
        "            if postag == lexicon[lemma][\"postag\"][0]:",
        "polarities.append(lexicon[lemma][\"polarity\"][0])",
        "else:",
        "polarities.append(lexicon[lemma][\"polarity\"][1])",
        "# add corresponding polarity to list of polarities",
        "    # lemma must be present in lexicon",
        "# only when the lemma and the POS-tag match, there is a match (see for example 'wrong' as ADJ and 'wrong' as NOUN)",
        "\n",
        "# review polarity",
        "polarity = sum(polarities)",
        "\n",
        "# final decision for this review",
        "if polarity > 0:",
        "    sentiment = \"positive\"",
        "elif polarity == 0:",
        "    sentiment = \"neutral\"",
        "elif polarity < 0:",
        "    sentiment = \"negative\"",
        "print(\"The polarity of the review is: \" + str(polarity))",
        "print(\"The sentiment of the review is \" + sentiment + \".\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div>\n",
        "    <font color=#690027 markdown=\"1\">\n",
        "<h2>4. Exercise: Sentiment lexicon matching on own review</h2>",
        "    </font>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can also do this for a self-written review and compare the system's output with your own annotation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# place self-written review between quotation marks, so adjust given string",
        "self_written_review = \"Hopefully this will be a fun notebook!\"",
        "# fill in polarity between quotation marks (positive, negative, neutral), so also adjust given string here",
        "label = \"positive\"",
        "\n",
        "# next steps: show review and apply nlp() to it",
        "print(user_written_review)",
        "review = nlp(self_written_review.lower())",
        "\n",
        "# show every word in review with word type and part-of-speech tag and save in lists",
        "tokens = []",
        "lemmas = []",
        "postags = []",
        "for token in review:",
        "tokens.append(token.text)",
        "lemmas.append(token.lemma_)",
        "    postags.append(token.pos_)",
        "\n",
        "print(\"tokens:\")",
        "print(tokens)",
        "print(\"lemmas:\")",
        "print(lemmas)",
        "print(\"part-of-speech tags:\")",
        "print(postags)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that the preprocessing is complete, you can search for matches with the lexicon again."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# search for matches with lexicon in review",
        "lexiconmatches = []       # empty list, to be filled with tokens of the lemmas found in lexicon",
        "polarities = []         # empty list, to be filled with polarities of found tokens",
        "\n",
        "# consider lemmas with corresponding word class and token",
        "for lemma, postag, token in zip(lemmas, postags, tokens):",
        "    if lemma in lexicon.keys() and postag in lexicon[lemma][\"postag\"]:",
        "lexiconmatches.append(token)                      # add corresponding token to the lexiconmatches list",
        "if postag == lexicon[lemma][\"postag\"][0]:",
        "polarities.append(lexicon[lemma][\"polarity\"][0])",
        "            else:",
        "polarities.append(lexicon[lemma][\"polarity\"][1])",
        "# add corresponding polarity to list of polarities",
        "    # lemma must be present in lexicon",
        "# only when the lemma and the POS-tag match, there is a match (see for example 'wrong' as ADJ and 'wrong' as NOUN)",
        "\n",
        "# review polarity",
        "polarity = sum(polarities)",
        "\n",
        "# final decision for this review",
        "if polarity > 0:",
        "    sentiment = \"positive\"",
        "elif polarity == 0:",
        "    sentiment = \"neutral\"",
        "elif polarity < 0:",
        "sentiment = \"negative\"",
        "print(\"The polarity of the review is: \" + str(polarity))",
        "print(\"The sentiment of the review is \" + sentiment + \".\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Compare the final decision of the rule-based system with your own annotation. Did the system get it right? Why/why not, do you think?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Answer:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src=\"images/cclic.png\" alt=\"Banner\" align=\"left\" width=\"100\"/><br><br>\n",
        "Notebook Chatbot, see <a href=\"http://www.aiopschool.be\">AI at School</a>, by C. Van Hee, V. Hoste, F. wyffels, Z. Van de Staey & N. Gesqui\u00e8re is licensed under a <a href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}