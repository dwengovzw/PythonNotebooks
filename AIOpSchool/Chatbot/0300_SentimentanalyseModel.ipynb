{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/bannerugentdwengo.png\" alt=\"BannerUGentDwengo\" width=\"250\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <font color=#690027 markdown=\"1\">\n",
    "        <h1>AI-MODEL VOOR SENTIMENTANALYSE</h1> \n",
    "    </font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-box alert-success\">\n",
    "In deze notebook zal je bij gegeven teksten (de data) onderzoek doen naar sentimentwoorden m.b.v. kunstmatige intelligentie (KI of AI). Je zal immers een <em>machine learning</em>-model gebruiken. Dit model werd getraind met geannoteerde teksten en kan met grote nauwkeurigheid een tekst tokeniseren en van elk token de part-of-speech tag en het lemma bepalen. Je gebruikt een <em>regelgebaseerd AI-systeem</em> om het sentiment van de gegeven tekst te bepalen. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In de vorige notebook, 'Regelgebaseerde sentimentanalyse', maakte je kennis met de principes van een regelgebaseerde sentimentanalyse: \n",
    "\n",
    " -  Je maakt gebruik van een (bestaand) **lexicon** of woordenboek met daarin woorden gekoppeld aan hun **polariteit** (positief, negatief of neutraal).\n",
    " -  Voor je sentimentwoorden uit een lexicon kunt matchen met de data moet je de data inlezen en **voorverwerken (preprocessing)**.\n",
    " -  Veelvoorkomende preprocessing stappen zijn **lowercasing**, **tokenisering**, **part-of-speech tagging** en  **lemmatisering**.\n",
    " \n",
    "In de vorige notebook waren nog niet alle stappen geautomatiseerd. Lemmatisering en part-of-speech tagging moesten manueel gebeuren. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-box alert-success\">\n",
    "In deze notebook zal je de uitvoer van de sentimentanalyse volledig <b>automatiseren</b>. Je zal m.a.w. de computer het werk laten doen: de computer zal de data voorverwerken met een <em>machine learning-model (ML-model)</em>, en met een <em>regelgebaseerd AI-systeem</em> de tokens matchen met het gegeven lexicon en een eindbeslissing nemen over het sentiment van de gegeven tekst. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modules, model en lexicon inladen "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voor je aan de slag gaat, voorzie je eerst de nodige tools:\n",
    "\n",
    "-  Je importeert de nodige modules (dit hoef je maar één keer te doen). <br>Deze modules bevatten functies en methodes die jouw onderzoek zullen vergemakkelijken. Er zijn immers reeds zaken voorgeprogrammeerd, waardoor jij met vrij eenvoudige instructies kunt werken.\n",
    "-  Je laadt een machine learning-model in om straks te gebruiken.\n",
    "-  Je leest ook al een sentimentlexicon in. \n",
    "\n",
    "Voer daartoe de drie code-cellen hieronder uit. De code in deze cellen hoef je niet te begrijpen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spacy installeren\n",
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modules importeren\n",
    "import pickle                     # voor lexicon\n",
    "from colorama import Fore, Back   # om in kleur te kunnen tonen\n",
    "import spacy                      # voor machine learning-model voor voorverwerking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset downloaden\n",
    "!python -m spacy download nl_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# machine learning-model inladen\n",
    "nlp = spacy.load(\"nl_core_news_sm\")    # nlp staat voor Natural Language Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lexicon inlezen, bestand 'new_lexicondict.pickle' bevat sentimentlexicon \n",
    "with open(\"data/new_lexicondict.pickle\", \"rb\") as file: \n",
    "    lexicon = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zo, je bent klaar voor stap 1: de data inlezen en bekijken. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <font color=#690027 markdown=\"1\">\n",
    "        <h2>1. De data inlezen</h2> \n",
    "    </font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voor deze opdracht zal je werken met dezelfde **klantenreview** als in de notebook 'Regelgebaseerde sentimentanalyse'. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stap 1: voer de volgende code-cel uit om de review in te lezen en vervolgens te bekijken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review = \"Nieuw concept in Gent, maar dat kan volgens mij toch beter. De meeste cornflakes waren gewoon de basic soorten. Ook wat duur voor de hoeveelheid die je krijgt, vooral met de toppings zijn ze zuinig. En als je ontbijt aanbiedt, geef de mensen dan toch ook wat meer keuze voor hun koffie.\"\n",
    "print(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Je bent klaar voor stap 2. \n",
    "\n",
    "In wat volgt laat je de computer al de voorverwerking op de review uitvoeren: lowercasing hadden we al geautomatiseerd in 'Regelgebaseerde sentimentanalyse'. Die code neem je over.  \n",
    "\n",
    "Je moet geen spaties toevoegen in de tekst, want het machine learning-model zorgt voor het tokeniseren. Ook het part-of-speech taggen en lemmatiseren worden nu geautomatiseerd m.b.v. het model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <font color=#690027 markdown=\"1\">\n",
    "        <h2>2. Preprocessing</h2> \n",
    "    </font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lowercasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zet tekst van review om naar tekst in kleine letters\n",
    "review_kleineletters = review.lower()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenisering, part-of-speech taggen en lemmatisering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De review **tokeniseren** en aan elk token een **part-of-speech** en een **lemma** toekennen, gebeurt automatisch met behulp van een daarvoor getraind model met een accuraatheid van 93 %! \n",
    "\n",
    "Je voert daarvoor de review (in kleine letters) in in het ML-model `nlp`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# review in kleine letters in model voeren\n",
    "review_voorverwerkt = nlp(review_kleineletters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Van de review zijn nu de tokens bepaald en van elk token is het woord (of leesteken) zelf, de woordsoort (part-of-speech tag) en de woordenboekvorm (lemma) opgeslagen in een object waarnaar wordt verwezen door `review_voorverwerkt`.  <br>\n",
    "    Je kan nu de kenmerken van tokens opvragen: het woord/leesteken via de instructie `token.text`, de woordsoort via `token.pos_` en de woordenboekvorm via `token.lemma_`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Van elk token de woordsoort en de woordenboekvorm tonen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tokens\n",
    "for token in review_voorverwerkt:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part-of-speech tag van elk token\n",
    "for token in review_voorverwerkt:\n",
    "    print(token.text + \": \" + token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemma van elk token\n",
    "for token in review_voorverwerkt:\n",
    "    print(token.text + \": \" + token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maak lijsten van de tokens, lemma's en part-of-speech tags."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In 'Regelgebaseerde sentimentanalyse' waren de lijsten van de lemma's en part-of-speech tags manueel opgemaakt. Nu kan dit automatisch omdat alle nodige info verzameld is in het object waarnaar de variabele `review_voorverwerkt` verwijst."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# oplijsten\n",
    "tokens = []\n",
    "lemmas = []\n",
    "postags = []\n",
    "for token in review_voorverwerkt:\n",
    "    tokens.append(token.text)      # voeg elk token toe aan lijst van tokens\n",
    "    lemmas.append(token.lemma_)    # voeg elk lemma toe aan lijst van lemma's\n",
    "    postags.append(token.pos_)     # voeg elke part-of-speech tag toe aan lijst van postags\n",
    "\n",
    "# lijsten tonen\n",
    "print(\"tokens:\")\n",
    "print(tokens)\n",
    "print(\"lemma's:\")\n",
    "print(lemmas)\n",
    "print(\"part-of-speech tags:\")\n",
    "print(postags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <font color=#690027 markdown=\"1\">\n",
    "        <h2>3. Sentiment lexicon matching</h2> \n",
    "    </font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nu de review *gepreprocessed* is, kan je het sentiment bepalen met behulp van het sentimentlexicon dat je ter beschikking hebt. Dit was reeds geautomatiseerd in 'Regelgebaseerde sentimentanalyse'. Je neemt de code van 'Regelgebaseerde sentimentanalyse' over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zoek matches met lexicon in review\n",
    "lexiconmatches = []       # lege lijst, op te vullen met tokens van de lemma's gevonden in lexicon\n",
    "polariteiten = []         # lege lijst, op te vullen met polariteiten van gevonden tokens \n",
    "\n",
    "# beschouw lemma's met overeenkomstige woordsoort en token\n",
    "for lemma, postag, token in zip(lemmas, postags, tokens):\n",
    "    if lemma in lexicon.keys() and postag in lexicon[lemma][\"postag\"]:  \n",
    "            lexiconmatches.append(token)                      # overeenkomstig token toevoegen aan lijst lexiconmatches\n",
    "            if postag == lexicon[lemma][\"postag\"][0]:\n",
    "                polariteiten.append(lexicon[lemma][\"polarity\"][0])\n",
    "            else:\n",
    "                polariteiten.append(lexicon[lemma][\"polarity\"][1])\n",
    "                # overeenkomstige polariteit toevoegen aan lijst polariteiten\n",
    "    # lemma moet aanwezig zijn in lexicon\n",
    "    # alleen wanneer het lemma en de POS-tag overeenkomen, is er een match (zie bv. 'fout' als ADJ en 'fout' als NOUN) \n",
    "\n",
    "# polariteit review\n",
    "polariteit = sum(polariteiten)\n",
    "\n",
    "# eindbeslissing voor deze review\n",
    "if polariteit > 0:\n",
    "    sentiment = \"positief\"\n",
    "elif polariteit == 0:\n",
    "    sentiment = \"neutraal\"\n",
    "elif polariteit < 0:\n",
    "    sentiment = \"negatief\"\n",
    "print(\"De polariteit van de review is: \" + str(polariteit))\n",
    "print(\"Het sentiment van de review is \" + sentiment + \".\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <font color=#690027 markdown=\"1\">\n",
    "        <h2>4. Oefening: Sentiment lexicon matching op eigen review</h2> \n",
    "    </font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Je kan dit ook doen voor een zelfgeschreven review en de output van het systeem vergelijken met je eigen annotatie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plaats zelfgeschreven review tussen aanhalingstekens, pas dus gegeven string aan\n",
    "zelfgeschreven_review = \"Hopelijk wordt dit een leuke notebook!\"\n",
    "# vul polariteit in tussen aanhalingstekens (positief, negatief, neutraal), pas dus ook hier gegeven string aan\n",
    "label = \"positief\"\n",
    "\n",
    "# volgende stappen: review tonen en nlp() erop toepassen\n",
    "print(zelfgeschreven_review)\n",
    "review = nlp(zelfgeschreven_review.lower())\n",
    "\n",
    "# elk woord in review tonen met woordsoort en part-of-speech tag en opslaan in lijsten\n",
    "tokens = []\n",
    "lemmas = []\n",
    "postags = []\n",
    "for token in review:\n",
    "    tokens.append(token.text)\n",
    "    lemmas.append(token.lemma_)\n",
    "    postags.append(token.pos_)\n",
    "\n",
    "print(\"tokens:\")\n",
    "print(tokens)\n",
    "print(\"lemma's:\")\n",
    "print(lemmas)\n",
    "print(\"part-of-speech tags:\")\n",
    "print(postags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nu de preprocessing klaar is, kan je opnieuw matches zoeken met het lexicon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zoek matches met lexicon in review\n",
    "lexiconmatches = []       # lege lijst, op te vullen met tokens van de lemma's gevonden in lexicon\n",
    "polariteiten = []         # lege lijst, op te vullen met polariteiten van gevonden tokens \n",
    "\n",
    "# beschouw lemma's met overeenkomstige woordsoort en token\n",
    "for lemma, postag, token in zip(lemmas, postags, tokens):\n",
    "    if lemma in lexicon.keys() and postag in lexicon[lemma][\"postag\"]:  \n",
    "            lexiconmatches.append(token)                      # overeenkomstig token toevoegen aan lijst lexiconmatches\n",
    "            if postag == lexicon[lemma][\"postag\"][0]:\n",
    "                polariteiten.append(lexicon[lemma][\"polarity\"][0])\n",
    "            else:\n",
    "                polariteiten.append(lexicon[lemma][\"polarity\"][1])\n",
    "                # overeenkomstige polariteit toevoegen aan lijst polariteiten\n",
    "    # lemma moet aanwezig zijn in lexicon\n",
    "    # alleen wanneer het lemma en de POS-tag overeenkomen, is er een match (zie bv. 'fout' als ADJ en 'fout' als NOUN) \n",
    "\n",
    "# polariteit review\n",
    "polariteit = sum(polariteiten)\n",
    "\n",
    "# eindbeslissing voor deze review\n",
    "if polariteit > 0:\n",
    "    sentiment = \"positief\"\n",
    "elif polariteit == 0:\n",
    "    sentiment = \"neutraal\"\n",
    "elif polariteit < 0:\n",
    "    sentiment = \"negatief\"\n",
    "print(\"De polariteit van de review is: \" + str(polariteit))\n",
    "print(\"Het sentiment van de review is \" + sentiment + \".\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vergelijk de eindbeslissing van het regelgebaseerde systeem met je eigen annotatie. Heeft het systeem het juist? Waarom wel/niet, denk je?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antwoord:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/cclic.png\" alt=\"Banner\" align=\"left\" width=\"100\"/><br><br>\n",
    "Notebook Chatbot, zie <a href=\"http://www.aiopschool.be\">AI Op School</a>, van C. Van Hee, V. Hoste, F. wyffels, Z. Van de Staey & N. Gesquière is in licentie gegeven volgens een <a href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\">Creative Commons Naamsvermelding-NietCommercieel-GelijkDelen 4.0 Internationaal-licentie</a>. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
